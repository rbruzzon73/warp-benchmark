
# run-warp-benchmark

## Notes run-warp-benchmark.sh:
- run-warp-benchmark.sh is based on Warp MinIO's S3 benchmarking tool designed to measure and analyze object storage performance. 
- The Warp tool generats realistic workloads and provides detailed performance metrics for S3-compatible storage systems.
- This script executes PUT and GET benchmarks that can be customized and adjusted to meet various requirements regarding size, duration, parallelism, and delay (sleep) between the two benchmark executions.  
- The operations are performed sequentially as detailed below:
   ~~~
   [1/8] Creating namespace 'warp-benchmark'...
   [2/8] Creating ObjectBucketClaim...
   [3/8] Waiting for OBC to be bound (provisioning bucket)...
   [4/8] Extracting credentials...
   [5/8] Deploying Warp Runner Pod...
   [6/8] Running PUT Benchmark...
   [7/8] Sleeping for 2m before GET benchmark...
   [8/8] Running GET Benchmark...
   ~~~



## Variables customizable in the run-warp-benchmark.sh script.
The run-warp-benchmark.sh support the customization of the following Variables groups: 

   - CONFIGURATION:
        ~~~
        NAMESPACE="warp-benchmark"
        OBC_NAME="warp-benchmark-bucket"
        STORAGE_CLASS="openshift-storage.noobaa.io"
        LOG_PREFIX="custom-run"
        ~~~

  - Warp Benchmark Settings:
       ~~~
       WARP_CONCURRENT=10         # Number of concurrent operations
       WARP_OBJ_SIZE="1536KiB"    # Object size
       WARP_DURATION_PUT="30s"    # Duration for PUT benchmark
       WARP_DURATION_GET="30s"    # Duration for GET benchmark
       WARP_GET_OBJECTS=500       # Number of objects for GET benchmark
       BENCHMARK_PAUSE="2m"       # Time to sleep between PUT and GET
       ~~~
       
## Output details:
- The output includes throughput measured in MB/s showing data transfer rates. 
- Operations per second display how many GET / PUT requests the system can handle. 
- Latency percentiles show response times at p50, p90, p99, and p99.9 levels for operations.
- TTFB is the time from request was sent to the first byte was received.

## Usefull links:
- [MinIO warp](https://docs.min.io/enterprise/minio-warp).
- [Throughput](https://docs.min.io/enterprise/minio-warp/reference/#throughput).
- [Operations per second](https://docs.min.io/enterprise/minio-warp/reference/#operations-per-second).
- [Latency percentiles](https://docs.min.io/enterprise/minio-warp/reference/#latency-metrics).

## Example of the output generated by run-warp-benchmark.sh
~~~
[root@dr-ocp-bastion-server ~]# ./run-warp-benchmark-v2.sh
--- Starting Warp Benchmark Automation ---
Settings: Concurrent=10, Size=1536KiB
Pause between benchmarks: 2m
[1/8] Creating namespace 'warp-benchmark'...
Namespace 'warp-benchmark' already exists. Switching to it.
Already on project "warp-benchmark" on server "https://api.ocp4-dr.test.com:6443".
[2/8] Creating ObjectBucketClaim...
objectbucketclaim.objectbucket.io/warp-benchmark-bucket unchanged
[3/8] Waiting for OBC to be bound (provisioning bucket)...
OBC is Bound.
[4/8] Extracting credentials...
Endpoint: s3.openshift-storage.svc
Bucket:   warp-bucket-8607a15b-d2be-4e62-bf01-cfa16760b73b
[5/8] Deploying Warp Runner Pod...
pod/warp-runner unchanged
Waiting for Pod to be Ready...
pod/warp-runner condition met
[6/8] Running PUT Benchmark...
---------------------------------------------------
>>> Starting PUT Benchmark (30s)...
    Saving to: custom-run-put-20260109-101745.json.zst
>>> Analyzing PUT Results:


Report: PUT (185 reqs). Ran Duration: 28s, starting 10:17:54 UTC
 * Objects per request: 1. Size: 1572864 bytes. Concurrency: 10.
 * Average: 8.98 MiB/s, 5.98 obj/s (28s)
 * Reqs: Avg: 1614.9ms, 50%: 1687.4ms, 90%: 1902.0ms, 99%: 2037.5ms, Fastest: 597.5ms, Slowest: 2451.8ms, StdDev: 262.9ms

Throughput, split into 28 x 1s:
 * Fastest: 10.3MiB/s, 6.85 obj/s (1s, starting 10:18:14 UTC)
 * 50% Median: 9.1MiB/s, 6.09 obj/s (1s, starting 10:17:56 UTC)
 * Slowest: 7.1MiB/s, 4.74 obj/s (1s, starting 10:17:52 UTC)


---------------------------------------------------
[7/8] Sleeping for 2m before GET benchmark...
Resuming...
---------------------------------------------------
[8/8] Running GET Benchmark...
>>> Starting GET Benchmark (30s)...
    Saving to: custom-run-get-20260109-102028.json.zst
>>> Analyzing GET Results:


Report: GET (527 reqs). Ran Duration: 27s, starting 10:21:21 UTC
 * Objects per request: 1. Size: 1572864 bytes. Concurrency: 10.
 * Average: 26.18 MiB/s, 17.45 obj/s (27s)
 * Reqs: Avg: 630.7ms, 50%: 636.9ms, 90%: 947.9ms, 99%: 1221.7ms, Fastest: 28.6ms, Slowest: 1417.0ms, StdDev: 245.0ms
 * TTFB: Avg: 576ms, Best: 23ms, 25th: 393ms, Median: 557ms, 75th: 774ms, 90th: 880ms, 99th: 1.157s, Worst: 1.389s StdDev: 232ms

Throughput, split into 27 x 1s:
 * Fastest: 36.1MiB/s, 24.10 obj/s (1s, starting 10:21:42 UTC)
 * 50% Median: 26.3MiB/s, 17.55 obj/s (1s, starting 10:21:43 UTC)
 * Slowest: 19.4MiB/s, 12.95 obj/s (1s, starting 10:21:24 UTC)


---------------------------------------------------
Benchmark Complete.
---------------------------------------------------

-------------------- NOTES ------------------------
---------------------------------------------------
The output includes throughput measured in MB/s showing data transfer rates.
Operations per second display how many GET / PUT requests the system can handle.
Latency percentiles show response times at p50, p90, p99, and p99.9 levels for operations.
Links:
Throughput -> https://docs.min.io/enterprise/minio-warp/reference/#throughput
Operations per second -> https://docs.min.io/enterprise/minio-warp/reference/#operations-per-second
Latency percentiles -> https://docs.min.io/enterprise/minio-warp/reference/#latency-metrics
~~~


