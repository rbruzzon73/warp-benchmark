
# run-warp-benchmark.sh

## Notes:
- run-warp-benchmark.sh is based on Warp MinIO's S3 benchmarking tool designed to measure and analyze object storage performance. 
- The Warp tool generats realistic workloads and provides detailed performance metrics for S3-compatible storage systems.
- This script executes PUT and GET benchmarks that can be customized and adjusted to meet various requirements regarding size, duration, parallelism, and delay (sleep) between the two benchmark executions.  
- The operations are performed sequentially as detailed below:
   ~~~
   [1/8] Creating namespace 'warp-benchmark'...
   [2/8] Creating ObjectBucketClaim...
   [3/8] Waiting for OBC to be bound (provisioning bucket)...
   [4/8] Extracting credentials...
   [5/8] Deploying Warp Runner Pod...
   [6/8] Running PUT Benchmark...
   [7/8] Sleeping for 2m before GET benchmark...
   [8/8] Running GET Benchmark...
   ~~~
## Variables customizable in the run-warp-benchmark.sh script.
The run-warp-benchmark.sh support the customization of the following Variables groups: 

   - CONFIGURATION:
        ~~~
        NAMESPACE="warp-benchmark"
        OBC_NAME="warp-benchmark-bucket"
        STORAGE_CLASS="openshift-storage.noobaa.io"
        LOG_PREFIX="custom-run"
        ~~~

  - Warp Benchmark Settings:
       ~~~
       WARP_CONCURRENT=10         # Number of concurrent operations
       WARP_OBJ_SIZE="1536KiB"    # Object size
       WARP_DURATION_PUT="30s"    # Duration for PUT benchmark
       WARP_DURATION_GET="30s"   # Duration for GET benchmark
       WARP_GET_OBJECTS=500       # Number of objects for GET benchmark
       BENCHMARK_PAUSE="2m"       # Time to sleep between PUT and GET
       ~~~
       
## Output details:
- The output includes throughput measured in MB/s showing data transfer rates. 
- Operations per second display how many GET / PUT requests the system can handle. 
- Latency percentiles show response times at p50, p90, p99, and p99.9 levels for operations.

## Usefull links:
- [MinIO warp](https://docs.min.io/enterprise/minio-warp).
- [Throughput](https://docs.min.io/enterprise/minio-warp/reference/#throughput).
- [Operations per second](https://docs.min.io/enterprise/minio-warp/reference/#operations-per-second).
- [Latency percentiles](https://docs.min.io/enterprise/minio-warp/reference/#latency-metrics).

## Example of the output generated by run-warp-benchmark.sh
~~~
[root@dr-ocp-bastion-server ~]# ./run-warp-benchmark.sh 
--- Starting Warp Benchmark Automation ---
Settings: Concurrent=10, Size=1536KiB
Pause between benchmarks: 2m
[1/7] Creating namespace 'warp-benchmark'...
Namespace 'warp-benchmark' already exists. Switching to it.
Already on project "warp-benchmark" on server "https://api.ocp4-dr.test.com:6443".
[2/7] Creating ObjectBucketClaim...
objectbucketclaim.objectbucket.io/warp-benchmark-bucket unchanged
[3/7] Waiting for OBC to be bound (provisioning bucket)...
OBC is Bound.
[4/7] Extracting credentials...
Endpoint: s3.openshift-storage.svc
Bucket:   warp-bucket-8607a15b-d2be-4e62-bf01-cfa16760b73b
[5/7] Deploying Warp Runner Pod...
pod/warp-runner unchanged
Waiting for Pod to be Ready...
pod/warp-runner condition met
[6/7] Running PUT Benchmark...
---------------------------------------------------
>>> Starting PUT Benchmark (30s)...
    Saving to: custom-run-put-20260109-094042.json.zst
>>> Analyzing PUT Results:


Report: PUT (260 reqs). Ran Duration: 28s, starting 09:40:47 UTC
 * Objects per request: 1. Size: 1572864 bytes. Concurrency: 10.
 * Average: 13.04 MiB/s, 8.69 obj/s (28s)
 * Reqs: Avg: 1186.3ms, 50%: 1149.3ms, 90%: 1701.9ms, 99%: 1942.0ms, Fastest: 534.6ms, Slowest: 2626.3ms, StdDev: 322.2ms

Throughput, split into 28 x 1s:
 * Fastest: 15.5MiB/s, 10.31 obj/s (1s, starting 09:40:46 UTC)
 * 50% Median: 13.4MiB/s, 8.96 obj/s (1s, starting 09:40:50 UTC)
 * Slowest: 8.6MiB/s, 5.76 obj/s (1s, starting 09:40:45 UTC)


---------------------------------------------------
Sleeping for 2m before GET benchmark...
Resuming...
---------------------------------------------------
[7/7] Running GET Benchmark...
>>> Starting GET Benchmark (30s)...
    Saving to: custom-run-get-20260109-094321.json.zst
>>> Analyzing GET Results:


Report: GET (265 reqs). Ran Duration: 27s, starting 09:44:27 UTC
 * Objects per request: 1. Size: 1572864 bytes. Concurrency: 10.
 * Average: 13.07 MiB/s, 8.71 obj/s (27s)
 * Reqs: Avg: 1154.4ms, 50%: 1176.4ms, 90%: 1533.1ms, 99%: 2081.8ms, Fastest: 282.2ms, Slowest: 2090.4ms, StdDev: 315.2ms
 * TTFB: Avg: 1.042s, Best: 228ms, 25th: 837ms, Median: 1.066s, 75th: 1.27s, 90th: 1.412s, 99th: 1.84s, Worst: 2.037s StdDev: 307ms

Throughput, split into 27 x 1s:
 * Fastest: 17.0MiB/s, 11.35 obj/s (1s, starting 09:44:47 UTC)
 * 50% Median: 13.0MiB/s, 8.69 obj/s (1s, starting 09:44:25 UTC)
 * Slowest: 9.8MiB/s, 6.55 obj/s (1s, starting 09:44:33 UTC)


---------------------------------------------------
Benchmark Complete.
---------------------------------------------------

-------------------- NOTES ------------------------
---------------------------------------------------
The output includes download throughput measured in MB/s showing data transfer rates.
Operations per second display how many GET requests the system can handle.
Latency percentiles show response times at p50, p90, p99, and p99.9 levels for operations.
Links:
Throughput -> https://docs.min.io/enterprise/minio-warp/reference/#throughput
Operations per second -> https://docs.min.io/enterprise/minio-warp/reference/#operations-per-second
Latency percentiles -> https://docs.min.io/enterprise/minio-warp/reference/#latency-metrics
~~~


